# -*- coding: utf-8 -*-
"""Hybrid Personal Data Assistant (structured + RAG)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xcCBDi1orMUEnIdJCpBRUEBXr0KzqRYi
"""

!pip install langchain langchain-community chromadb sentence-transformers gradio pypdf pandas openpyxl langchain-openai beautifulsoup4 requests

from langchain_community.document_loaders import PyPDFLoader

loader=PyPDFLoader(r"/content/wellarchitected-framework.pdf")
docs=loader.load()

docs

len(docs)

from langchain_community.document_loaders.onedrive_file import CHUNK_SIZE
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_spliter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)

texts=text_spliter.split_documents(docs)


texts

from langchain_community.embeddings import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(
    model_name="BAAI/bge-base-en-v1.5"
)

from langchain_community.vectorstores import Chroma

vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embedding,
    persist_directory="chroma_db"
)

retriever = vectordb.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 6, "fetch_k": 20}
)

import pandas as pd

transactions_df = pd.read_excel("synthetic_transactions_clean.xlsx")

# IMPORTANT: ensure Date is datetime
transactions_df["Date"] = pd.to_datetime(transactions_df["Date"])

transactions_df.head()

import re
import pandas as pd

def answer_transactions(query: str, df: pd.DataFrame):
    q = query.lower()

    # ðŸ”¹ payments above X
    match = re.search(r"(above|near|min)\s*(\d+)", q)
    if match and "payment" in q:
        threshold = float(match.group(2))

        filtered = df[df["Amount"] > threshold]

        if filtered.empty:
            return "No payments found above that amount."

        return filtered[["Date","Time","Amount","Merchant"]].to_string(index=False)

    # ðŸ”¹ total spending
    if "total" in q and ("spend" in q or "payment" in q):
        total = df["Amount"].sum()
        return f"Your total spending is â‚¹{total:,.2f}"

    # âœ… SIMPLE DATE QUERY (YOUR NEW FEATURE)
    date_match = re.search(r"\d{4}-\d{2}-\d{2}", q)
    if date_match:
        date_str = date_match.group(0)
        target_date = pd.to_datetime(date_str).date()

        filtered = df[df["Date"].dt.date == target_date]

        if filtered.empty:
            return f"No transactions found on {date_str}."

        total = filtered["Amount"].sum()

        return (
            f"On {date_str}, you made {len(filtered)} transactions "
            f"totalling â‚¹{total:,.2f}.\n\n"
            + filtered[["Date","Time","Amount","Merchant"]].to_string(index=False)
        )

    # ðŸ”¹ today's payments
    if "today" in q:
        today = pd.Timestamp.today().date()
        filtered = df[df["Date"].dt.date == today]

        if filtered.empty:
            return "No transactions found for today."

        return filtered.to_string(index=False)

    return None

from google.colab import userdata
import os

GOOGLE_API_KEY = userdata.get('project1')

!pip install langchain-google-genai
from langchain_google_genai import ChatGoogleGenerativeAI

llm=ChatGoogleGenerativeAI(model="gemini-2.5-flash",google_api_key=GOOGLE_API_KEY)

###from langchain_openai import ChatOpenAI

#llm = ChatOpenAI(
 #   model="sarvam-m",
  #  base_url="https://api.sarvam.ai/v1",
   # api_key="sk_0q78n4l9_49WWGZzZScPlS6IOtX0net9d", # Replace with your actual API key
    #temperature=0.2
#)###

from langchain_classic.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

def route_query(query: str) -> str:
    q = query.lower()

    transaction_keywords = [
        "payment","transaction","spent",
        "amount","paid","expense","above","total"
    ]

    if any(k in q for k in transaction_keywords):
        return "transactions"

    return "rag"

def smart_assistant(query: str):
    route = route_query(query)

    if route == "transactions":
        answer = answer_transactions(query, transactions_df)
        if answer:
            return answer

    # fallback to RAG
    result = qa_chain.invoke({"query": query})
    return result["result"]

import gradio as gr
import pandas as pd

# ðŸ”¹ load your transactions file
transactions_df = pd.read_excel("synthetic_transactions_clean.xlsx")
transactions_df["Date"] = pd.to_datetime(transactions_df["Date"])

# ðŸ”¹ main Gradio wrapper
def gradio_chat(query):
    try:
        return smart_assistant(query)
    except Exception as e:
        return f"Error: {str(e)}"

# ðŸ”¹ UI
demo = gr.Interface(
    fn=gradio_chat,
    inputs=gr.Textbox(
        placeholder="Ask about your payments or documents...",
        label="Smart Finance + RAG Assistant"
    ),
    outputs=gr.Textbox(
        label="Response",
        lines=18,
        max_lines=30
    ),

    title="Multi-Source Personal AI Assistant",
    description="Query transactions or documents using hybrid routing (Rules + RAG)."
)

if __name__ == "__main__":
    demo.launch()

print(smart_assistant("payment above 20000"))

print(smart_assistant("Summarize about General design principles"))